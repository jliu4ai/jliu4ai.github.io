<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>FunHSI: Open-Vocabulary Functional 3D Human-Scene Interaction Generation</title>
  <meta name="description" content="FunHSI: Training-free open-vocabulary functional 3D human-scene interaction generation from posed RGB-D and a task prompt." />

  <link rel="icon" href="assets/img/favicon.png" />
  <link rel="stylesheet" href="assets/css/style.css" />

  <!-- OpenGraph -->
  <meta property="og:title" content="FunHSI: Open-Vocabulary Functional 3D Human-Scene Interaction Generation" />
  <meta property="og:description" content="Training-free, functionality-driven 3D human-scene interaction generation from posed RGB-D observations and open-vocabulary task prompts." />
  <meta property="og:image" content="assets/img/teaser.jpg" />
  <meta property="og:type" content="website" />
</head>

<body>
  <header class="site-header">
    <div class="container">
      <nav class="topnav">
        <a class="brand" href="#">FunHSI</a>
        <div class="navlinks">
          <a href="#abstract">Abstract</a>
          <a href="#video">Video</a>
          <a href="#method">How It Works</a>
          <a href="#results">Results</a>
          <a href="#citation">Citation</a>
        </div>
      </nav>
    </div>
  </header>

  <main class="container">
    <!-- Hero -->
    <section class="hero">
      <h1 class="title">Open-Vocabulary Functional 3D Human-Scene Interaction Generation</h1>

      <div class="authors">
        <p>
          <a href="https://jliu4ai.github.io/" target="_blank" rel="noopener">Jie Liu</a><sup>1,2</sup>,
          <a href="https://www.yusun.work/" target="_blank" rel="noopener">Yu Sun</a><sup>1</sup>,
          <a href="https://www.linkedin.com/in/alp%C3%A1r-cseke-b8784a179/" target="_blank" rel="noopener">Alpar Cseke</a><sup>3</sup>,
          <a href="https://yfeng95.github.io/" target="_blank" rel="noopener">Yao Feng</a><sup>4</sup>,
          <a href="https://www.linkedin.com/in/nicolasheron/" target="_blank" rel="noopener">Nicolas Heron</a><sup>1</sup>,
          <a href="https://is.mpg.de/ps/person/black" target="_blank" rel="noopener">Michael J. Black</a><sup>3</sup>,
          <a href="https://yz-cnsdqz.github.io/" target="_blank" rel="noopener">Yan Zhang</a><sup>1</sup>
        </p>

        <p class="affiliations">
          <span><sup>1</sup>Meshcapade</span>
          <span class="dot">•</span>
          <span><sup>2</sup>University of Amsterdam</span>
          <br />
          <span class="dot">•</span>
          <span><sup>3</sup>Max Planck Institute for Intelligent Systems</span>
          <span class="dot">•</span>
          <span><sup>4</sup>Stanford University</span>
        </p>
      </div>

      <div class="buttons">
        <a class="btn" href="assets/files/paper.pdf" target="_blank" rel="noopener">Paper</a>
        <a class="btn" href="https://youtu.be/0wKSViA_39A" target="_blank" rel="noopener">Video</a>
        <a class="btn" href="https://youtu.be/YOUR_VIDEO_ID" target="_blank" rel="noopener">Code</a>
        <a class="btn" href="https://github.com/YOUR_GITHUB/REPO" target="_blank" rel="noopener">Poster</a>
      </div>

      <p class="tagline">
        FunHSI is a training-free framework that generates physically plausible and functionally correct 3D human–scene interactions
        from posed RGB-D observations and open-vocabulary task prompts.
      </p>

      <figure class="figure">
        <video
          class="teaser-video"
          src="/publication_jie/FunHSI_Demo_Final.mp4"
          autoplay
          loop
          muted
          playsinline
          controls
        ></video>
        <figcaption>
          <strong>FunHSI.</strong> Demo video showcasing functionality-aware 3D human–scene interactions across diverse scenes.
        </figcaption>
      </figure>
    </section>

    <!-- Abstract (from paper) -->
    <section id="abstract" class="section">
      <h2>Abstract</h2>
      <p>
        Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in
        embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics
        of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction.
        Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding
        human-scene contact, resulting in implausible or functionally incorrect interactions. We propose FunHSI, a training-free,
        functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts.
        Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements,
        reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models
        to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed
        3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness.
        FunHSI supports both general interactions (e.g., “sitting on a sofa”) and fine-grained functional interactions
        (e.g., “increasing the room temperature”), and consistently generates functionally correct and physically plausible interactions
        across diverse indoor and outdoor scenes.
      </p>
    </section>

    <!-- Video -->
    <section id="video" class="section">
      <h2>Video</h2>
      <div class="video-grid">
        <div class="video">
          <iframe
            src="https://www.youtube-nocookie.com/embed/0wKSViA_39A"
            title="FunHSI video"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen
          ></iframe>
        </div>
      </div>
    </section>

    <!-- Method -->
    <section id="method" class="section">
      <h2>How It Works</h2>
      <p>
        FunHSI takes a set of posed RGB-D images and an open-vocabulary task prompt as input, and generates a 3D human
        that accomplishes the specified task via functionally correct contact with the scene. The pipeline consists of three
        modules: (i) <strong>Functionality-aware Contact Reasoning</strong> detects task-relevant functional elements, lifts 2D masks into 3D,
        and uses LLM-based <em>contact graph reasoning</em> to infer body–scene contact relations;
        (ii) <strong>Functionality-aware Body Initialization</strong> synthesizes a human in the image, estimates SMPL-X body pose and hand pose,
        and refines the contact graph to resolve left–right ambiguity; (iii) <strong>Body Refinement</strong> performs two-stage optimization
        with collision and contact objectives to ensure physical plausibility and functional correctness.
      </p>

      <figure class="figure">
        <img src="assets/img/method.jpg" alt="FunHSI method overview" />
        <figcaption>
          <strong>Pipeline overview.</strong> FunHSI: functionality grounding + contact graph reasoning,
          contact-aware human initialization, and two-stage refinement for stable contacts and low penetration.
        </figcaption>
      </figure>

      <div class="steps">
        <div class="step">
          <div class="step-num">1</div>
          <div class="step-body">
            <div class="step-title">Functionality-aware Contact Reasoning</div>
            <div class="step-text">
              Use VLMs to infer functional elements from the task prompt, segment them in 2D, lift to 3D, and build an LLM-predicted contact graph.
            </div>
          </div>
        </div>
        <div class="step">
          <div class="step-num">2</div>
          <div class="step-body">
            <div class="step-title">Body Initialization</div>
            <div class="step-text">
              Contact-aware human inpainting + generator–critic loop reduces hallucination; estimate SMPL-X body pose and hand pose, then refine contact laterality.
            </div>
          </div>
        </div>
        <div class="step">
          <div class="step-num">3</div>
          <div class="step-body">
            <div class="step-title">Two-stage Body Refinement</div>
            <div class="step-text">
              Optimize global alignment and arm reach first, then fine-tune full-body stability with collision, contact, and pose priors.
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Results -->
    <section id="results" class="section">
      <h2>Results</h2>
      <p>
        FunHSI achieves strong performance on a benchmark derived from SceneFun3D under both
        <strong>general HSI</strong> and <strong>functional HSI</strong> settings. It substantially improves functional contact distance compared to
        adapted GenZI and GenHSI baselines, while maintaining competitive semantic consistency and non-collision scores.
        A perceptual user study shows that participants strongly prefer FunHSI outputs over baselines, with higher preference margins
        in functional tasks that require reasoning about fine-grained functional elements.
      </p>

      <figure class="figure">
        <img src="assets/img/results.jpg" alt="FunHSI results figure" />
        <figcaption>
          <strong>Qualitative results.</strong> FunHSI robustly grounds interactions on task-relevant functional elements (e.g., knobs, handles, switches)
          and produces physically plausible contacts across diverse scenes.
        </figcaption>
      </figure>

      <!-- 你也可以把下面的“数字”改成你最终图里展示的版本 -->
      <div class="steps" style="margin-top:16px;">
        <div class="step">
          <div class="step-num">★</div>
          <div class="step-body">
            <div class="step-title">User Study Preference</div>
            <div class="step-text">
              Overall preference (vs. baselines) is strong; functional tasks show larger margins than general tasks.
              (Replace this line with the exact numbers shown in your final figure.)
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Citation -->
    <section id="citation" class="section">
      <h2>Citation</h2>
      <p>If you find this work useful, please consider citing:</p>
      <pre class="bibtex"><code>@article{liu2026funhsi,
  title   = {Open-Vocabulary Functional 3D Human-Scene Interaction Generation},
  author  = {Liu, Jie and Sun, Yu and Cseke, Alpar and Feng, Yao and Heron, Nicolas and Black, Michael J. and Zhang, Yan},
  journal = {arXiv preprint},
  year    = {2026}
}</code></pre>
    </section>

    <!-- Acknowledgements -->
    <section class="section">
      <h2>Acknowledgements</h2>
      <p>
        We sincerely thank Alexandros Delitzas and Francis Engelmann for guidance on SceneFun3D; Priyanka Patel for guidance on CameraHMR;
        and Muhammed Kocabas for fruitful discussions on foundation models. We also thank Nitin Saini and Nathan Bajandas for help with Unreal Engine.
        This work was done when Jie Liu was an intern at Meshcapade.
      </p>
      <p class="last-updated">Last updated: January, 2026</p>
    </section>

    <footer class="footer">
      <p>© <span id="year"></span> FunHSI authors. Project page hosted on GitHub Pages.</p>
    </footer>
  </main>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
