<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>FunHSI: Open-Vocabulary Functional 3D Human-Scene Interaction Generation</title>
  <meta name="description" content="FunHSI: Training-free open-vocabulary functional 3D human-scene interaction generation from posed RGB-D and a task prompt." />

  <link rel="icon" href="assets/img/favicon.png" />
  <link rel="stylesheet" href="assets/css/style.css" />

  <!-- OpenGraph -->
  <meta property="og:title" content="FunHSI: Open-Vocabulary Functional 3D Human-Scene Interaction Generation" />
  <meta property="og:description" content="Training-free, functionality-driven 3D human-scene interaction generation from posed RGB-D observations and open-vocabulary task prompts." />
  <meta property="og:image" content="assets/img/teaser.jpg" />
  <meta property="og:type" content="website" />
</head>

<body>
  <header class="site-header">
    <div class="container">
      <nav class="topnav">
        <a class="brand" href="#">FunHSI</a>
        <div class="navlinks">
          <a href="#abstract">Abstract</a>
          <a href="#video">Video</a>
          <a href="#method">How It Works</a>
          <a href="#results">Results</a>
          <a href="#citation">Citation</a>
        </div>
      </nav>
    </div>
  </header>

  <main class="container">
    <!-- Hero -->
    <section class="hero">
      <h1 class="title">Open-Vocabulary Functional 3D Human-Scene Interaction Generation</h1>

      <div class="authors">
        <p>
          <a href="https://jliu4ai.github.io/" target="_blank" rel="noopener">Jie Liu</a><sup>1,2</sup>,
          <a href="https://www.yusun.work/" target="_blank" rel="noopener">Yu Sun</a><sup>1</sup>,
          <a href="https://www.linkedin.com/in/alp%C3%A1r-cseke-b8784a179/" target="_blank" rel="noopener">Alpár Cseke</a><sup>3</sup>,
          <a href="https://yfeng95.github.io/" target="_blank" rel="noopener">Yao Feng</a><sup>4</sup>,
          <a href="https://www.linkedin.com/in/nicolasheron/" target="_blank" rel="noopener">Nicolas Heron</a><sup>1</sup>,
          <a href="https://is.mpg.de/ps/person/black" target="_blank" rel="noopener">Michael J. Black</a><sup>3</sup>,
          <a href="https://yz-cnsdqz.github.io/" target="_blank" rel="noopener">Yan Zhang</a><sup>1</sup>
        </p>

        <p class="affiliations">
          <span><sup>1</sup>Meshcapade</span>
          <span class="dot">•</span>
          <span><sup>2</sup>University of Amsterdam</span>
          <br />
          <span class="dot">•</span>
          <span><sup>3</sup>Max Planck Institute for Intelligent Systems</span>
          <span class="dot">•</span>
          <span><sup>4</sup>Stanford University</span>
        </p>
      </div>

      <div class="buttons">
        <a class="btn" href="https://arxiv.org/pdf/2601.20835" target="_blank" rel="noopener">Paper</a>
        <a class="btn" href="https://youtu.be/0wKSViA_39A" target="_blank" rel="noopener">Video</a>
        <a class="btn" href="https://youtu.be/YOUR_VIDEO_ID" target="_blank" rel="noopener">Code</a>
        <a class="btn" href="https://github.com/YOUR_GITHUB/REPO" target="_blank" rel="noopener">Poster</a>
      </div>

      <p class="tagline">
        FunHSI is a training-free framework that generates physically plausible and functionally correct 3D human–scene interactions
        from posed RGB-D observations and open-vocabulary task prompts.
      </p>

      <figure class="figure">
        <video
          class="teaser-video"
          src="/publication_jie/FunHSI_Demo_Final.mp4"
          autoplay
          loop
          muted
          playsinline
          controls
        ></video>
        <figcaption>
          <strong>FunHSI.</strong> Demo video showcasing functionality-aware 3D human–scene interactions across diverse scenes.
        </figcaption>
      </figure>
    </section>

    <!-- Abstract (from paper) -->
    <section id="abstract" class="section">
      <h2>Abstract</h2>
      <p>
        Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in
        embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics
        of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction.
        Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding
        human-scene contact, resulting in implausible or functionally incorrect interactions. We propose FunHSI, a training-free,
        functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts.
        Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements,
        reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models
        to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed
        3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness.
        FunHSI supports both general interactions (e.g., “sitting on a sofa”) and fine-grained functional interactions
        (e.g., “increasing the room temperature”), and consistently generates functionally correct and physically plausible interactions
        across diverse indoor and outdoor scenes.
      </p>
    </section>

    <!-- Video -->
    <section id="video" class="section">
      <h2>Video</h2>
      <div class="video-grid">
        <div class="video">
          <iframe
            src="https://www.youtube-nocookie.com/embed/0wKSViA_39A"
            title="FunHSI video"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen
          ></iframe>
        </div>
      </div>
    </section>

    <!-- Method -->
    <section id="method" class="section">
      <h2>How It Works</h2>
      <figure class="figure">
        <img src="assets/img/funhsi_framework.png" alt="FunHSI method overview" />
        <figcaption>
          FunHSI takes a set of posed RGB-D images and an open-vocabulary task prompt as input, and generates a 3D human
          that accomplishes the specified task via functionally correct contact with the scene. The pipeline consists of three
          modules: (i) <strong>Functionality-aware Contact Reasoning</strong>,
          (ii) <strong>Functionality-aware Body Initialization</strong> , and (iii) <strong>Two-stage Body Refinement</strong>.
        </figcaption>
      </figure>

      <div class="steps">
        <div class="step">
          <div class="step-num">1</div>
          <div class="step-body">
            <div class="step-title">Functionality-aware Contact Reasoning</div>
            <div class="step-text">
              This module identifies task-relevant functional elements in the scene, reconstructs their 3D geometry, and performs contact graph reasoning to produce the high-level interactions.
            </div>
          </div>
        </div>
        <div class="step">
          <div class="step-num">2</div>
          <div class="step-body">
            <div class="step-title">Contact-aware Body Initialization</div>
            <div class="step-text">
              This module leverages the inferred functional elements and contact relations to synthesize a human in the image and estimate the 3D body and the hand poses
            </div>
          </div>
        </div>
        <div class="step">
          <div class="step-num">3</div>
          <div class="step-body">
            <div class="step-title">Two-stage Body Refinement</div>
            <div class="step-text">
             This module places the initialized 3D body into the 3D scene and performs stage-wise optimization to refine the body and hand poses, and human-scene contacts.
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Results -->
    <section id="hsi_results" class="section">
      <h2>General Human-Scene Interaction</h2>
      <div class="results-grid">
        <div class="results-item">
          <p class="results-label">Squatting in front of washing machine</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_006.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Walking in front of the left wooden cabinet</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_007.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
      </div>

    </section>

    <section id="fhsi_results" class="section">
      <h2>Functional Human-Scene Interaction</h2>
      <div class="results-grid">
        <div class="results-item">
          <p class="results-label">Adjusting the temperature</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_008.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Opening the bottom drawer of the leftmost wooden cabinet with the books on top</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_001.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Opening the window to the left of the couch</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_002.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Adjusting the room's temperature using the dial next to the door</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_003.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Switching to a station on the radio on the bedside table</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_004.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Opening the top drawer of the wooden nightstand to the left of the bed</p>
          <video
            class="results-video"
            src="assets/video/blender_scenefun3d_005.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
      </div>

    </section>

    <section id="cityscene_results" class="section">
      <h2>Human-Scene Interaction in Real-world City Scenes</h2>
      <p class="section-subtitle">(These scenes were captured from Munich using iPhone 14 Pro Max.)</p>
      <div class="results-grid">
        <div class="results-item">
          <p class="results-label">Buying a parking ticket</p>
          <video
            class="results-video"
            src="assets/video/blender_city_000.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Decorating the Christmas tree</p>
          <video
            class="results-video"
            src="assets/video/blender_city_001.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Pinning a paper to the whiteboard</p>
          <video
            class="results-video"
            src="assets/video/blender_city_002.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Buying a metro ticket</p>
          <video
            class="results-video"
            src="assets/video/blender_city_003.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Opening the emergency door</p>
          <video
            class="results-video"
            src="assets/video/blender_city_004.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
        <div class="results-item">
          <p class="results-label">Sitting on a bench</p>
          <video
            class="results-video"
            src="assets/video/blender_city_005.mp4"
            autoplay
            loop
            muted
            playsinline
          ></video>
        </div>
      </div>

    </section>


    <!-- Citation -->
    <section id="citation" class="section">
      <h2>Citation</h2>
      <p>If you find this work useful, please consider citing:</p>
      <pre class="bibtex"><code>@misc{liu2026openvocabularyfunctional3dhumanscene,
        title={Open-Vocabulary Functional 3D Human-Scene Interaction Generation}, 
        author={Jie Liu and Yu Sun and Alpar Cseke and Yao Feng and Nicolas Heron and Michael J. Black and Yan Zhang},
        year={2026},
        eprint={2601.20835},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2601.20835}, 
  }</code></pre>
    </section>

    <!-- Acknowledgements -->
    <section class="section">
      <h2>Acknowledgements</h2>
      <p>
        We sincerely thank Alexandros Delitzas and Francis Engelmann for guidance on SceneFun3D; Priyanka Patel for guidance on CameraHMR;
        and Muhammed Kocabas for fruitful discussions on foundation models. We also thank Nitin Saini and Nathan Bajandas for help with Unreal Engine.
        This work was done when Jie Liu was an intern at Meshcapade.
      </p>
      <p class="last-updated">Last updated: January, 2026</p>
    </section>

    <footer class="footer">
      <p>© <span id="year"></span> FunHSI authors. Project page hosted on GitHub Pages.</p>
    </footer>
  </main>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
